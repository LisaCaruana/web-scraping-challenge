{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from splinter import Browser\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time \n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - ====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 89.0.4389\n",
      "[WDM] - Get LATEST driver version for 89.0.4389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Driver [C:\\Users\\carua.DESKTOP-4FTJ629\\.wdm\\drivers\\chromedriver\\win32\\89.0.4389.23\\chromedriver.exe] found in cache\n"
     ]
    }
   ],
   "source": [
    "# Establish a path with Chrome Driver and create browser\n",
    "executable_path={'executable_path': ChromeDriverManager().install()}\n",
    "browser=Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign URL and visit with browser\n",
    "url= \"https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest\"\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an HTML object\n",
    "html = browser.html\n",
    "# Parse HTML with Beautiful Soup\n",
    "soup = bs(html, 'html.parser')\n",
    "# Scrape the [NASA Mars News Site](https://mars.nasa.gov/news/) and collect the latest News Title and Paragraph Text. Assign the text to variables.\n",
    "news_title = soup.find_all(\"div\", class_=\"content_title\")[1].text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and print first paragraph\n",
    "news_p = soup.find(\"div\", class_=\"article_teaser_body\").text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load URL for image and visit using splinter \n",
    "image_url= \"https://www.jpl.nasa.gov/images?search=&category=Mars\"\n",
    "browser.visit(image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the latest featured image using splinter and click on it\n",
    "time.sleep(1)\n",
    "browser.find_by_css('.mb-3').click()\n",
    "html = browser.html\n",
    "\n",
    "# Parse HTML with Beautiful Soup\n",
    "soup = bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate image using splinter, assign to a variable, and save and print complete URL\n",
    "featured_img= soup.find(\"img\", class_=\"BaseImage\")\n",
    "featured_img_url= featured_img['data-src']\n",
    "time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Mars Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visit the Mars Facts webpage (https://space-facts.com/mars/) and assign to variable name\n",
    "facts_url= \"https://space-facts.com/mars/\"\n",
    "browser.visit(facts_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read table on webpage using Pandas and assign to variable name\n",
    "mars_facts=pd.read_html(facts_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe using scraped data\n",
    "facts_df = mars_facts[0]\n",
    "\n",
    "# Rename columns and set 'Category' column as index\n",
    "facts_df.columns= (['Category', 'Mars Facts'])\n",
    "mars_facts_clean=facts_df.drop(0)\n",
    "clean_df=mars_facts_clean.set_index('Category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Pandas to convert the data to a HTML table string and print as a file.\n",
    "html_table = facts_df.to_html('mars.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Pandas to convert the data to a HTML table string to use in Mongo dictionary.\n",
    "html_table_mongo = facts_df.to_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Mars Hemispheres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visit the USGS Astrogeology site\n",
    "astrogeo_url= \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "browser.visit(astrogeo_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an HTML object\n",
    "html = browser.html\n",
    "\n",
    "# Parse HTML with Beautiful Soup\n",
    "soup = bs(html, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape the USGS Astrogeology site and collect high resolution images for each of Mar's hemispheres. \n",
    "# Save both the image url string for the full resolution hemisphere image and the Hemisphere title containing the hemisphere name. \n",
    "\n",
    "# Starting with first hemisphere listed, click on hemisphere name\n",
    "cerb_url=browser.links.find_by_partial_text('Cerberus')\n",
    "cerb_url.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sleep browser for one second and create a new HTML object on page\n",
    "time.sleep(1)\n",
    "html = browser.html\n",
    "# Parse Cerberus page HTML with Beautiful Soup\n",
    "soup_cerb = bs(html, 'html.parser')\n",
    "\n",
    "# Using BeautifulSoup, locate and save hemisphere image and title \n",
    "cerb_photo = soup_cerb.find(\"a\", text=\"Sample\").get(\"href\")\n",
    "cerb_title = soup_cerb.find(\"h2\", class_=\"title\").text\n",
    "browser.back()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = browser.html\n",
    "# Parse HTML with Beautiful Soup\n",
    "soup = bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "schia_url=browser.links.find_by_partial_text('Schiaparelli')\n",
    "schia_url.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(1)\n",
    "html = browser.html\n",
    "# Parse HTML with Beautiful Soup\n",
    "soup_schia = bs(html, 'html.parser')\n",
    "schia_photo = soup_schia.find(\"a\", text=\"Sample\").get(\"href\")\n",
    "schia_title = soup_schia.find(\"h2\", class_=\"title\").text\n",
    "#print(schia_photo)\n",
    "browser.back()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = browser.html\n",
    "# Parse HTML with Beautiful Soup\n",
    "soup = bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "syrtis_url=browser.links.find_by_partial_text('Syrtis')\n",
    "syrtis_url.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(1)\n",
    "html = browser.html\n",
    "# Parse HTML with Beautiful Soup\n",
    "soup_syrtis = bs(html, 'html.parser')\n",
    "syrtis_photo = soup_syrtis.find(\"a\", text=\"Sample\").get(\"href\")\n",
    "syrtis_title = soup_syrtis.find(\"h2\", class_=\"title\").text\n",
    "#print(syrtis_photo)\n",
    "browser.back()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = browser.html\n",
    "# Parse HTML with Beautiful Soup\n",
    "soup = bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "valles_url=browser.links.find_by_partial_text('Valles')\n",
    "valles_url.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(1)\n",
    "html = browser.html\n",
    "# Parse HTML with Beautiful Soup\n",
    "soup_valles = bs(html, 'html.parser')\n",
    "valles_photo = soup_valles.find(\"a\", text=\"Sample\").get(\"href\")\n",
    "valles_title = soup_valles.find(\"h2\", class_=\"title\").text\n",
    "#print(valles_photo)\n",
    "browser.back()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pymongo\n",
    "\n",
    "# # Connect to Mongo\n",
    "# # This needs to be done in python \n",
    "# mongo_conn='mongodb://localhost:27017'\n",
    "# client=pymongo.MongoClient(mongo_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hemisphere_image_urls=[\n",
    "#     {\"title\": cerb_title, \"img_url\": cerb_photo},\n",
    "#     {\"title\": schia_title, \"img_url\": schia_photo},\n",
    "#     {\"title\": syrtis_title, \"img_url\": syrtis_photo},\n",
    "#     {\"title\": valles_title, \"img_url\": valles_photo}\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Store table and headlines in dictionary\n",
    "# mars_facts= {\n",
    "#     'Headline': news_title,\n",
    "#     'News': news_p,\n",
    "#     'Featured Image': featured_img_url,\n",
    "#     'Table': html_table_mongo,\n",
    "#     'Hemisphere Images': hemisphere_image_urls\n",
    "#     }\n",
    "# # print(mars_facts)\n",
    "# # Import to MongoDB\n",
    "# client.mars_db.mars.insert_one(mars_facts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test code or unworkable/broken Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#want to grab text of URL and store into dictionary\n",
    "#cerb_photo=cerb_url.find_by_partial_text('Sample').click()\n",
    "#cerb_photo.click()\n",
    "# Need to put code for all hemispheres into a loop; exercise stu_scrape_weather\n",
    "# Put elements into a dictionary \n",
    "# Try craigslist exercises \n",
    "# facts_df.reset_index(name='Facts about Mars', drop=True)\n",
    "# facts_df.rename(columns= {\"0\": \"Facts\",\"1\": \"On Mars\"})\n",
    "# mars_facts\n",
    "# type(mars_facts)\n",
    "# facts_df.index_col=[0]\n",
    "# for col in facts_df.columns:\n",
    "# print(col)\n",
    "# soup = bs(html, 'html.parser')\n",
    "# soup= bs(response.text, 'html.parser')\n",
    "# soup\n",
    "# articles = soup.find_all('article', class_='product_pod')\n",
    "# print(soup)\n",
    "# facts_df.pd(header=False, index=False)\n",
    "# browser.cerb_url.find_by_text('Sample').click()\n",
    "#cerb_hem = soup.find_all(\"div\", class_=\"h3\")[1].jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# featured_image = soup.find_all(\"div\", class_=\"content_title\")[1].text\n",
    "# print(news_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn = 'mongodb://localhost:27017'\n",
    "# import pymongo\n",
    "# PyMongo constructor - this is a class, as noted by the capital letters\n",
    "# client = pymongo.MongoClient(conn)\n",
    "# db=client.MarsDB\n",
    "# response= requests.get(url)\n",
    "# facts_df.drop(\"index\")\n",
    "# facts_df.set_index=(['Category'])\n",
    "#Store hemisphere labels and images in a dictionary. Use a Python dictionary to store the data using the keys `img_url` and `title`.\n",
    "# hemisphere_image_urls=[\n",
    "#     {\"title\": cerb_title, \"img_url\": cerb_photo},\n",
    "#     {\"title\": schia_title, \"img_url\": schia_photo},\n",
    "#     {\"title\": syrtis_title, \"img_url\": syrtis_photo},\n",
    "#     {\"title\": valles_title, \"img_url\": valles_photo}\n",
    "  \n",
    "# ]\n",
    "# #print(hemisphere_image_urls)\n",
    "\n",
    "# Import to MongoDB\n",
    "# client.mars_db.mars.insert_one(hemisphere_image_urls)\n",
    "# Put all 4 in a loop, will click into first one, scrape the website, grab text and full size image, store those\n",
    "#i n a dictionary, then goes to next link do same thing, etc. 4 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
