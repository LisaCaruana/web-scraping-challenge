{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from splinter import Browser\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time \n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - ====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 89.0.4389\n",
      "[WDM] - Get LATEST driver version for 89.0.4389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Driver [C:\\Users\\carua.DESKTOP-4FTJ629\\.wdm\\drivers\\chromedriver\\win32\\89.0.4389.23\\chromedriver.exe] found in cache\n"
     ]
    }
   ],
   "source": [
    "# Establish a path with Chrome Driver and create browser\n",
    "executable_path={'executable_path': ChromeDriverManager().install()}\n",
    "browser=Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign URL and visit with browser\n",
    "url= \"https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest\"\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Another First: Perseverance Captures the Sounds of Driving on Mars\n"
     ]
    }
   ],
   "source": [
    "# Create an HTML object\n",
    "html = browser.html\n",
    "# Parse HTML with Beautiful Soup\n",
    "soup = bs(html, 'html.parser')\n",
    "#Scrape the [NASA Mars News Site](https://mars.nasa.gov/news/) and collect the latest News Title and Paragraph Text. Assign the text to variables that you can reference later.\n",
    "news_title = soup.find_all(\"div\", class_=\"content_title\")[1].text\n",
    "# Print headline\n",
    "print(news_title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NASA’s newest rover recorded audio of itself crunching over the surface of the Red Planet, adding a whole new dimension to Mars exploration.\n"
     ]
    }
   ],
   "source": [
    "# Find and print first paragraph\n",
    "news_p = soup.find(\"div\", class_=\"article_teaser_body\").text\n",
    "print(news_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load URL for image and visit using splinter \n",
    "image_url= \"https://www.jpl.nasa.gov/images?search=&category=Mars\"\n",
    "browser.visit(image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.find_by_css('.mb-3').click()\n",
    "html = browser.html\n",
    "# Parse HTML with Beautiful Soup\n",
    "soup = bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featured_image_url: https://d2pn8kiwq2w21t.cloudfront.net/images/jpegPIA24504.2e16d0ba.fill-400x400-c50.jpg\n"
     ]
    }
   ],
   "source": [
    "# Locate image using splinter, assign to a variable, and save and print complete URL\n",
    "featured_img= soup.find(\"img\", class_=\"BaseImage\")\n",
    "featured_img_url= featured_img['data-src']\n",
    "print(f\"Featured_image_url: {featured_img_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Mars Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visit the Mars Facts webpage [here](https://space-facts.com/mars/) \n",
    "#and use Pandas to scrape the table containing facts about the planet \n",
    "#including Diameter, Mass, etc.\n",
    "facts_url= \"https://space-facts.com/mars/\"\n",
    "browser.visit(facts_url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read table on webpage using Pandas and assign to variable name\n",
    "mars_facts=pd.read_html(facts_url)\n",
    "#mars_facts\n",
    "# mars_facts\n",
    "# type(mars_facts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mars Facts</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Polar Diameter:</th>\n",
       "      <td>6,752 km</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mass:</th>\n",
       "      <td>6.39 × 10^23 kg (0.11 Earths)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Moons:</th>\n",
       "      <td>2 (Phobos &amp; Deimos)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Orbit Distance:</th>\n",
       "      <td>227,943,824 km (1.38 AU)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Orbit Period:</th>\n",
       "      <td>687 days (1.9 years)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Surface Temperature:</th>\n",
       "      <td>-87 to -5 °C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>First Record:</th>\n",
       "      <td>2nd millennium BC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recorded By:</th>\n",
       "      <td>Egyptian astronomers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Mars Facts\n",
       "Category                                           \n",
       "Polar Diameter:                            6,752 km\n",
       "Mass:                 6.39 × 10^23 kg (0.11 Earths)\n",
       "Moons:                          2 (Phobos & Deimos)\n",
       "Orbit Distance:            227,943,824 km (1.38 AU)\n",
       "Orbit Period:                  687 days (1.9 years)\n",
       "Surface Temperature:                   -87 to -5 °C\n",
       "First Record:                     2nd millennium BC\n",
       "Recorded By:                   Egyptian astronomers"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataframe using scraped data\n",
    "facts_df = mars_facts[0]\n",
    "#facts_df.reset_index(name='Facts about Mars', drop=True)\n",
    "# facts_df.rename(columns= {\"0\": \"Facts\",\"1\": \"On Mars\"})\n",
    "facts_df.columns= (['Category', 'Mars Facts'])\n",
    "mars_facts_clean=facts_df.drop(0)\n",
    "clean_df=mars_facts_clean.set_index('Category')\n",
    "\n",
    "#facts_df.drop(\"index\")\n",
    "#facts_df.set_index=(['Category'])\n",
    "clean_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use Pandas to convert the data to a HTML table string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_table = facts_df.to_html('mars.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Mars Hemispheres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visit the USGS Astrogeology site\n",
    "astrogeo_url= \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "browser.visit(astrogeo_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = browser.html\n",
    "# Parse HTML with Beautiful Soup\n",
    "soup = bs(html, 'html.parser')\n",
    "#print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an HTML object\n",
    "\n",
    "#Scrape the USGS Astrogeology site and collect high resolution images for each of Mar's hemispheres. \n",
    "#Save both the image url string for the full resolution hemisphere image, \n",
    "#and the Hemisphere title containing the hemisphere name. \n",
    "# Use a Python dictionary to store the data using the keys `img_url` and `title`.\n",
    "cerb_url=browser.links.find_by_partial_text('Cerberus')\n",
    "cerb_url.click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cerberus Hemisphere Enhanced\n"
     ]
    }
   ],
   "source": [
    "time.sleep(1)\n",
    "html = browser.html\n",
    "# Parse HTML with Beautiful Soup\n",
    "soup_cerb = bs(html, 'html.parser')\n",
    "cerb_photo = soup_cerb.find(\"a\", text=\"Sample\").get(\"href\")\n",
    "cerb_title = soup_cerb.find(\"h2\", class_=\"title\").text\n",
    "print(cerb_title)\n",
    "#browser.back()\n",
    "#print(cerb_photo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#want to grab text of URL and store into dictionary\n",
    "#cerb_photo=cerb_url.find_by_partial_text('Sample').click()\n",
    "#cerb_photo.click()\n",
    "# Need to put code for all hemispheres into a loop; exercise stu_scrape_weather\n",
    "# Put elements into a dictionary \n",
    "# Try craigslist exercises "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = browser.html\n",
    "# Parse HTML with Beautiful Soup\n",
    "soup = bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "schia_url=browser.links.find_by_partial_text('Schiaparelli')\n",
    "schia_url.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(1)\n",
    "html = browser.html\n",
    "# Parse HTML with Beautiful Soup\n",
    "soup_schia = bs(html, 'html.parser')\n",
    "schia_photo = soup_schia.find(\"a\", text=\"Sample\").get(\"href\")\n",
    "schia_title = soup_schia.find(\"h2\", class_=\"title\").text\n",
    "#print(schia_photo)\n",
    "browser.back()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = browser.html\n",
    "# Parse HTML with Beautiful Soup\n",
    "soup = bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "syrtis_url=browser.links.find_by_partial_text('Syrtis')\n",
    "syrtis_url.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(1)\n",
    "html = browser.html\n",
    "# Parse HTML with Beautiful Soup\n",
    "soup_syrtis = bs(html, 'html.parser')\n",
    "syrtis_photo = soup_syrtis.find(\"a\", text=\"Sample\").get(\"href\")\n",
    "syrtis_title = soup_syrtis.find(\"h2\", class_=\"title\").text\n",
    "#print(syrtis_photo)\n",
    "browser.back()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = browser.html\n",
    "# Parse HTML with Beautiful Soup\n",
    "soup = bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "valles_url=browser.links.find_by_partial_text('Valles')\n",
    "valles_url.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(1)\n",
    "html = browser.html\n",
    "# Parse HTML with Beautiful Soup\n",
    "soup_valles = bs(html, 'html.parser')\n",
    "valles_photo = soup_valles.find(\"a\", text=\"Sample\").get(\"href\")\n",
    "valles_title = soup_valles.find(\"h2\", class_=\"title\").text\n",
    "#print(valles_photo)\n",
    "browser.back()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all 4 in a loop, will click into first one, scrape the website, grab text and full size image, store those\n",
    "#in a dictionary, then goes to next link do same thing, etc. 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cerb_hem = soup.find_all(\"div\", class_=\"h3\")[1].jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# browser.cerb_url.find_by_text('Sample').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test code or unworkable/broken Code\n",
    "#facts_df.index_col=[0]\n",
    "# for col in facts_df.columns:\n",
    "#     print(col)\n",
    "#soup = bs(html, 'html.parser')\n",
    "# soup= bs(response.text, 'html.parser')\n",
    "# soup\n",
    " # articles = soup.find_all('article', class_='product_pod')\n",
    "#print(soup)\n",
    "#facts_df.pd(header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#featured_image = soup.find_all(\"div\", class_=\"content_title\")[1].text\n",
    "#print(news_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn = 'mongodb://localhost:27017'\n",
    "# import pymongo\n",
    "# #PyMongo constructor - this is a class, as noted by the capital letters\n",
    "# client = pymongo.MongoClient(conn)\n",
    "\n",
    "# db=client.MarsDB\n",
    "# response= requests.get(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
